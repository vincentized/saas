import streamlit as st
import openai
import pdfplumber
from docx import Document
import re
import nltk
nltk.download('punkt')
from nltk.tokenize import sent_tokenize

# Set your OpenAI API key here
api_key = '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'
openai.api_key = api_key

# def wrap_text_with_sentences(text, max_tokens):
#     sentences = sent_tokenize(text)
#     response = ""
#     token_count = 0

#     for sentence in sentences:
#         sentence_tokens = len(openai.Completion.create(engine="text-davinci-003", prompt=sentence)["choices"][0]["text"].split())
#         if token_count + sentence_tokens <= max_tokens:
#             response += sentence + " "
#             token_count += sentence_tokens
#         else:
#             break

#     # Ensure the last sentence is complete
#     if response.strip() and not re.search(r'[.!?]$', response.strip()):
#         # Find the last sentence and remove it from the response
#         last_sentence = sentences[-1]
#         response = response[:-len(last_sentence)]

#     return response.strip()

# def wrap_text_with_sentences(text, max_tokens):
#     sentences = sent_tokenize(text)
#     response = ""
#     token_count = 0

#     for sentence in sentences:
#         sentence_tokens = len(openai.Completion.create(engine="text-davinci-003", prompt=sentence)["choices"][0]["text"].split())
#         if token_count + sentence_tokens <= max_tokens:
#             response += sentence + " "
#             token_count += sentence_tokens
#         else:
#             break

#     # Ensure the last sentence is complete
#     if response.strip() and not re.search(r'[.!?]$', response.strip()):
#         # Find the last sentence and remove it from the response
#         last_sentence = sentences[-1]
#         response = response[:-len(last_sentence)]

#     return response.strip()

#Sample Correspondence and Sample Response
sample_correspondence = """
[Sample Correspondence]
Ref: ABC/XXX/2021

Date: 1 October 2021

Company XYZ Ltd
XXX Duxton Rd, Singapore XXX XXX

Attn: Mr. Knot Guilt

Dear Sir, 

Request Subcontractors to avoid damaging the stone at Level One Lobby Area

It has come to our attention installed stones at level one lobby area were damaged and the respective Subcontractor has reported to us that you caused such damage by way of not providing necessary protection and protective coverage to already installed and completed works by others at the area where your works are currently being carried out.

We request the Subcontractor to exercise due care at all times to avoid damaging completed works by others and to provide necessary protection and protective coverage to already installed and completed works by others.

We enclose herein the damage reports generated by other Subcontractor for your review, verification, and reply with necessary justification if any (see the Enclosure) for our further review and investigation.

Should you have any inquiries or wish to call for three-party meeting, please do not hesitate to contact the undersigned.

In the meanwhile, we reserve all our rights under the Subcontract and at law.

Best Regards,

Sam Constructwell 
Project Manager
Company ABC

Encl.
Damage Reports for the Stone Works at Level 1 Lobby Area
"""

sample_response = """
[Sample Response]
Ref: XYZ/XXX/2021

Date: 3 October 2021

Company ABC Ltd
XXX Tanjong Pagar Rd, Singapore XXX XXX

Attn: Mr. Constructwell,

RE: Request Subcontractors to avoid damaging the stone at Level One Lobby Area

Dear Sir,

Thank you for bringing this matter to our attention. We sincerely apologize for any damage caused to the stones in the level one lobby area. We take this matter seriously, and we are committed to rectifying the situation.

We have reviewed the damage reports provided, and we acknowledge our responsibility in ensuring the protection of completed works. We will take immediate steps to provide the necessary protection and coverage to prevent further damage to the area.

Additionally, we will conduct a thorough investigation to identify the root causes of the damage and implement corrective measures to prevent such incidents in the future. Our goal is to maintain the highest standards of care and professionalism in our work.

Please be assured that we will cooperate fully with your team in resolving this issue promptly. We understand the importance of adhering to contractual obligations and ensuring the successful completion of the project.

If you require any further information or if you would like to discuss this matter in detail, please feel free to contact me directly.

Best regards,
Knot Guilt
Project Supervisor
XYZ Ltd
"""

# Streamlit UI
st.title("Construction Correspondence Management")

# Text input for the user to interact with the virtual assistant
user_input = st.text_input("You:", "")

# File uploader for PDF and Word files
uploaded_file = st.file_uploader("Upload a PDF or Word file:", type=["pdf", "docx"])

# Buttons for generating a summary and a reply
if st.button("Generate Summary"):
    user_input = "Give me an executive summary for this uploaded file highlighting all the critical content including requested action, deadline, responsible party, indicator for potential legal action if any"

if st.button("Generate Reply"):
    user_input = "Give me a reply letter for this uploaded file"

# Initialize variables to store extracted file text and chat history
file_text = ""
chat_history = []

# Determine the file type and extract text accordingly
if uploaded_file is not None:
    st.write("You've uploaded a file:", uploaded_file.name)

    if uploaded_file.type == "application/pdf":
        with pdfplumber.open(uploaded_file) as pdf:
            for page in pdf.pages:
                file_text += page.extract_text()

    elif uploaded_file.type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
        doc = Document(uploaded_file)
        file_text = " ".join([p.text for p in doc.paragraphs])

# Check if user input is not empty
if user_input.strip():
    # Append user input to chat history
    chat_history.append({"role": "user", "content": user_input})

    # Append extracted file text to chat history
    if file_text:
        chat_history.append({"role": "user", "content": file_text})

    # Create a list of messages for the chat
    messages = [{"role": "system", "content": "You are a helpful assistant for construction correspondence management."}] + chat_history

    # Determine max tokens based on the type of response
    if "reply" in user_input.lower() or "response" in user_input.lower() or "answer" in user_input.lower():
        max_tokens = 500
    else:
        max_tokens = 120

    # Use st.spinner() to indicate work in progress
    with st.spinner("Generating Response..."):
        # Generate a response from the virtual assistant using gpt-3.5-turbo engine
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=messages,
            max_tokens=max_tokens,  # Adjust this as needed
            temperature=0.2
        )

    # # Wrap the response within max tokens with complete sentences
    # wrapped_response = wrap_text_with_sentences(response.choices[0].message["content"], max_tokens)

    # # Append the virtual assistant's response to chat history
    # chat_history.append({"role": "assistant", "content": wrapped_response})


#  # Wrap the response within max tokens with complete sentences
#     wrapped_response = wrap_text_with_sentences(response['choices'][0]['message']['content'], max_tokens)

    #Append the virtual assistant's response to chat history
    chat_history.append({"role": "assistant", "content": response.choices[0].message["content"]})

    # # Append the virtual assistant's response to chat history
    # chat_history.append({"role": "assistant", "content": wrapped_response})

    # Logging: Append the conversation history to a log file for monitoring and analysis
    with open("chat_log.txt", "a") as log_file:
        for message in messages:
            log_file.write(f"{message['role']}: {message['content']}\n")
        log_file.write(f"Assistant: {response.choices[0].message['content']}\n")

# Display only the current response in the answer box
if chat_history:
    st.text_area("Virtual Assistant:", chat_history[-1]["content"], height=100)


